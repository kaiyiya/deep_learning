幻灯片第1页：标题页
大家好，今天我将介绍一种名为“纠正式检索增强生成（CRAG）”的创新方法。这是一种轻量化解决方案，旨在提升大语言模型的事实准确性与鲁棒性。接下来，我将从研究背景、方法设计到实验验证，全面解析CRAG的核心价值。

幻灯片第2页：研究背景与核心挑战
当前大语言模型（LLM）在生成质量和推理能力上表现突出，但面临两大核心挑战：
LLM的双重困境：频繁产生“幻觉”现象，生成不实信息；事实性错误难以预测和控制；知识更新滞后于现实世界。
RAG的局限性：检索增强生成虽能改善知识任务表现，但对检索质量高度敏感，容错性极低，且缺乏对检索结果的质量判断机制。这些问题直接影响了LLM在实际应用中的可靠性。

幻灯片第3页：问题定义
CRAG的核心问题可以形式化定义为：给定输入 ( x ) 和语料库 ( C )，检索器 ( R ) 返回 top-K 文档集合 ( D )，生成器 ( G ) 基于输入 ( x ) 和检索文档 ( D ) 生成输出 ( y )。
关键在于：当检索文档 ( D ) 质量不理想时，如何提高生成概率 ( P(y|x) ) 的鲁棒性与准确性？这正是CRAG要解决的核心问题。

幻灯片第4页：CRAG方法概览
CRAG是一种插件式、轻量化的校正机制，可无缝集成到现有RAG系统中。它通过智能评估、动态决策与知识精炼三大步骤，显著提升生成质量。接下来，我将详细拆解其核心创新点。


幻灯片第5页：CRAG核心创新
CRAG包含四大核心创新：
检索评估器：轻量级评估器对检索结果进行相关性与可信度评分；
置信度判断：基于评分分类为Correct、Incorrect、Ambiguous三种状态；
动态策略：不同置信度触发不同处理策略；
知识精炼：通过分解-重组机制过滤噪声，提取高质量知识片段。
这些创新共同构成了CRAG的鲁棒性基础。


幻灯片第6页：检索评估器设计
检索评估器的核心功能是对每个（查询，文档）对计算细粒度相关性分数。其技术特点包括：
使用轻量级模型控制计算开销；
可通过微调适配特定领域；
输出连续分数支持灵活决策；
同时支持文档级和段落级评估。
评估器减少了对单一检索结果的盲目信任，为后续决策提供了可靠依据。


幻灯片第7页：三级置信度判断机制
CRAG通过三级置信度判断机制动态调整策略：
CORRECT：检索结果总体可信，仅执行内部知识精炼；
INCORRECT：检索结果不可信，触发外部Web搜索；
AMBIGUOUS：部分可信，同时进行内部精炼和外部检索。
这种分类机制确保了系统在不同场景下的适应性。


幻灯片第8页：知识精炼：分解-重组策略
知识精炼采用分解-重组策略，分为三步：
文档分段：将检索文档切分为独立的知识片段（strips）；
评分过滤：使用评估器对每个片段打分，保留Top-k段落；
有序重组：按原文顺序重新组合筛选后的段落。
这一机制有效剔除干扰信息，显著提高上下文相关性。


幻灯片第9页：外部Web搜索增强

当内部检索不足时，CRAG启动外部搜索补充机制：
使用GPT-3.5 Turbo生成精简搜索关键词；
通过Google Search API执行大规模网络检索；
优先选择维基百科等高质量来源。
核心优势在于克服静态语料库的覆盖局限与时效问题，引入更全面的外部知识。


幻灯片第10页：CRAG推理流程
CRAG的推理流程分为五步：
评估打分 → 2. 置信度判定 → 3. 策略选择 → 4. 知识整合 → 5. 生成输出。
例如，当置信度为Correct时，系统仅执行内部精炼；为Incorrect时，则触发外部搜索。这种动态流程确保了生成结果的准确性。

幻灯片第11页：系统架构的可插拔性
CRAG采用插件式设计，核心组件均可替换：
评估器 ( E ) 可适配不同领域；
重写器 ( W ) 支持多种查询改写策略；
生成器 ( G ) 兼容LLaMA、GPT等主流模型。
这种设计使CRAG能无缝集成到现有RAG框架中，无需重构整体架构。


幻灯片第12页：实现细节与超参数配置
在实现上，CRAG的关键配置包括：
知识分段规则：短文本保持单段，长文本按句子切分；
评估器配置：内部精炼Top-k=5，阈值=-0.5；
外部搜索配置：优先抓取维基百科页面；
生成模型：支持LLaMA2、SelfRAG等。
这些参数可根据任务需求灵活调整。

幻灯片第13页：实验设置
为验证CRAG的有效性，我们设计了以下实验：
评测数据集：PopQA、Biography、PubHealth、ARC-Challenge；
对比基线：Standard RAG、Self-RAG、SAIL等；
评价指标：Accuracy、FactScore等。
实验覆盖了短文本问答、长文本生成及推理任务。


幻灯片第14页：实验结果：显著性能提升
实验结果显示，CRAG在多项任务上显著优于基线：
PopQA准确率提升8.8%；
Biography事实性改善12.5%；
ARC推理增强7.2%。
此外，Self-CRAG（CRAG + Self-RAG）进一步提升了性能，证明了方法的强可扩展性。


幻灯片第15页：跨模型鲁棒性验证
我们在LLaMA2、SelfRAG、Alpaca、ChatGPT等模型上测试了CRAG的普适性。结果显示，所有模型均显示一致的性能改进，证明CRAG的通用性。这一特性使其适用于不同规模和架构的语言模型。


幻灯片第16页：消融实验分析
消融实验验证了各组件的贡献：
移除评估器导致性能下降15%；
去除知识精炼机制损失22%准确率；
禁用Web搜索降低18%性能。
这些结果证明三级置信度判断策略的有效性，且各组件缺一不可。


幻灯片第17页：方法优势
CRAG的核心优势可总结为四点：
插件式设计：部署灵活，兼容多种模型；
鲁棒性提升：主动补救检索错误；
噪声过滤：精炼机制减少无关信息干扰；
事实准确性：显著降低幻觉现象。
这些优势使CRAG成为生产环境中可靠的解决方案。


幻灯片第18页：局限性与挑战
尽管CRAG表现优异，但仍面临三方面挑战：
计算开销：额外评估与搜索增加延迟；
评估器质量：依赖高质量标注数据；
外部依赖：Web搜索受网络和API限制。
未来需通过模型蒸馏、缓存优化等技术解决这些问题。


幻灯片第19页：未来研究方向
论文提出四个未来方向：
评估器优化：探索自监督训练降低标注依赖；
连续决策机制：扩展到更细粒度的策略空间；
方法融合：结合链式思考、工具调用等技术；
在线学习：引入用户反馈构建闭环系统。
这些方向将进一步推动CRAG的发展。


幻灯片第20页：核心贡献总结
最后，CRAG的核心贡献可总结为：
通过检索评估、置信度判断、知识精炼与外部搜索四大机制，显著提升RAG的鲁棒性与事实准确性；
插件化设计使其可扩展到不同模型与任务；
为生产环境中降低LLM幻觉风险提供了可行路径。
谢谢大家！欢迎提问与讨论。
